{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "\n",
    "# This is for the progress bar.\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from dataset import SpineDataset_Test\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#from model import unet_model , \n",
    "from models.U2net import U2NET\n",
    "from models.Unet import unet_model\n",
    "\n",
    "t1 = A.Compose([\n",
    "    A.augmentations.transforms.CLAHE(),\n",
    "    A.Resize(1024,512),\n",
    "    ToTensorV2()\n",
    "])\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_1_30_AP.txt\")as f:\n",
    "    d  = f.read().splitlines()\n",
    "for i in d:\n",
    "    tmp = \"./testing/\" + i\n",
    "    data.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SpineDataset_Test(data,t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = unet_model().to(DEVICE)\n",
    "model.load_state_dict(torch.load(f\"checkpoint/unet/0.6607674749561321_best_resnetlarge.ckpt\"))\n",
    "# model = U2NET().to(DEVICE)\n",
    "# model.load_state_dict(torch.load(f\"checkpoint/axial/0.8863151219611287_best_resnetlarge.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unet_model(\n",
       "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv1): encoding_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv2): encoding_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv3): encoding_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv4): encoding_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv5): encoding_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv6): encoding_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv7): encoding_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv8): encoding_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (tconv1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (tconv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (tconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (tconv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (bottleneck): encoding_block(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (final_layer): Conv2d(64, 9, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kettt = 0\n",
    "# visualize = np.zeros((3,1024,512))\n",
    "#     0:  [255, 255, 255],\n",
    "# cls_color = {\n",
    "#     1:  [0 , 0 , 255],\n",
    "#     2:  [0, 153, 0],\n",
    "#     3:  [57, 217, 249],\n",
    "#     4:  [255, 128, 0],\n",
    "#     5:  [204, 153, 255],\n",
    "#     6: [255, 255, 0],\n",
    "# }\n",
    "# with torch.no_grad():\n",
    "#     for x,path_name in test_batch:\n",
    "#         write_name = path_name[0].split(\"/\")[-1]\n",
    "#         print(write_name)\n",
    "#         if kettt==1:\n",
    "#             print(\"break\")\n",
    "#             break\n",
    "#         softmax = nn.Softmax(dim=1)\n",
    "#         x = x.float().to(DEVICE)\n",
    "#         preds = torch.argmax(softmax(model(x)),axis=1).to('cpu')\n",
    "#         uni_data = np.unique(preds)\n",
    "#         #if(len(uni_data)!=8):\n",
    "#         if (len(uni_data)==7 and 7 not in uni_data):\n",
    "#             print(path_name)\n",
    "#             #fig , ax =  plt.subplots(1,2 , figsize=(4, 8))\n",
    "\n",
    "#             img1 = np.transpose(np.array(x[0,:,:,:].to('cpu')),(1,2,0))\n",
    "#             preds1 = np.array(preds[0,:,:])\n",
    "            \n",
    "#             img1 = cv2.cvtColor(img1 , cv2.COLOR_GRAY2BGR)\n",
    "#             img1 = np.array(img1).transpose(2,0,1)\n",
    "#             #preds1 = ((preds1/np.max(preds1))*255).astype(np.uint8)\n",
    "#             for i in range(len(preds1)):\n",
    "#                 for j in range(len(preds1[0])):\n",
    "#                     if(preds1[i][j]==0):\n",
    "#                         continue\n",
    "#                     #print(preds1[i][j])\n",
    "#                     cmap = cls_color[preds1[i][j]]\n",
    "#                     for index in range(3):\n",
    "#                         img1[index][i][j] = cmap[index]\n",
    "#             img1 = img1.transpose(1,2,0)\n",
    "#             cv2.imwrite(f\"./result/pass/{write_name}\",img1)\n",
    "#             print(path_name)\n",
    "\n",
    "#             #ax[0].set_title('Image')\n",
    "#             #ax[1].set_title('Prediction')\n",
    "#             #ax[0].imshow(img1)\n",
    "#             #ax[1].imshow(preds1)\n",
    "#             print(uni_data)\n",
    "#             kettt+=1\n",
    "#             #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kettt = 0\n",
    "visualize = np.zeros((3,1024,512))\n",
    "cls_color = {\n",
    "    0:  [255, 255, 255],\n",
    "    1:  [0 , 0 , 255],\n",
    "    2:  [0, 153, 0],\n",
    "    3:  [57, 217, 249],\n",
    "    4:  [255, 128, 0],\n",
    "    5:  [204, 153, 255],\n",
    "    6: [255, 255, 0],\n",
    "}\n",
    "with torch.no_grad():\n",
    "    for x,path_name in test_batch:\n",
    "        write_name = path_name[0].split(\"/\")[-1]\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        x = x.float().to(DEVICE)\n",
    "        preds = torch.argmax(softmax(model(x)),axis=1).to('cpu')\n",
    "        uni_data = np.unique(preds)\n",
    "        img1 = np.transpose(np.array(x[0,:,:,:].to('cpu')),(1,2,0))\n",
    "        preds1 = np.array(preds[0,:,:])\n",
    "        \n",
    "        img1 = cv2.cvtColor(img1 , cv2.COLOR_GRAY2BGR)\n",
    "        img1 = np.array(img1).transpose(2,0,1)\n",
    "        for i in range(len(preds1)):\n",
    "            for j in range(len(preds1[0])):\n",
    "                if(preds1[i][j]==0):\n",
    "                    continue\n",
    "                #print(preds1[i][j])\n",
    "                cmap = cls_color[preds1[i][j]]\n",
    "                for index in range(3):\n",
    "                    img1[index][i][j] = cmap[index]\n",
    "        img1 = img1.transpose(1,2,0)\n",
    "        if (len(uni_data)==7):\n",
    "            cv2.imwrite(f\"./AP_result_1_30/pass/{write_name}\",img1)\n",
    "        else:\n",
    "            cv2.imwrite(f\"./AP_result_1_30/notp/{write_name}\",img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kettt = 0\n",
    "# visualize = np.zeros((3,1024,512))\n",
    "# cls_color = {\n",
    "#     0:  [255, 255, 255],\n",
    "#     1:  [0 , 0 , 255],\n",
    "#     2:  [0, 153, 0],\n",
    "#     3:  [57, 217, 249],\n",
    "#     4:  [255, 128, 0],\n",
    "#     5:  [204, 153, 255],\n",
    "#     6: [255, 255, 0],\n",
    "#     7: [255,0,255],\n",
    "#     8: [0,255,0]\n",
    "# }\n",
    "# with torch.no_grad():\n",
    "#     for x,path_name in test_batch:\n",
    "#         write_name = path_name[0].split(\"/\")[-1]\n",
    "#         softmax = nn.Softmax(dim=1)\n",
    "#         x = x.float().to(DEVICE)\n",
    "#         preds = torch.argmax(softmax(model(x)),axis=1).to('cpu')\n",
    "#         uni_data = np.unique(preds)\n",
    "#         img1 = np.transpose(np.array(x[0,:,:,:].to('cpu')),(1,2,0))\n",
    "#         preds1 = np.array(preds[0,:,:])\n",
    "        \n",
    "#         img1 = cv2.cvtColor(img1 , cv2.COLOR_GRAY2BGR)\n",
    "#         img1 = np.array(img1).transpose(2,0,1)\n",
    "#         for i in range(len(preds1)):\n",
    "#             for j in range(len(preds1[0])):\n",
    "#                 if(preds1[i][j]==0):\n",
    "#                     continue\n",
    "#                 #print(preds1[i][j])\n",
    "#                 cmap = cls_color[preds1[i][j]]\n",
    "#                 for index in range(3):\n",
    "#                     img1[index][i][j] = cmap[index]\n",
    "#         img1 = img1.transpose(1,2,0)\n",
    "#         if (len(uni_data)==9):\n",
    "#             cv2.imwrite(f\"./LA_result_1_9/pass/3-{write_name}\",img1)\n",
    "#         else:\n",
    "#             cv2.imwrite(f\"./LA_result_1_9/notp/3-{write_name}\",img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f47d3238610>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = visualize.transpose(1,2,0)\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_data.txt\",'r') as f:\n",
    "    data = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./FO-258218491416838431_json',\n",
       " './FO-112139868677800148_json',\n",
       " './FO-256791920264072115_json',\n",
       " './FO-328335381031203413_json',\n",
       " './FO-359284993441545309_json',\n",
       " './FO-410298935991288946_json',\n",
       " './FO-466248626243795334_json',\n",
       " './FO-233184483539218267_json',\n",
       " './FO-133976320887897066_json',\n",
       " './FO-156825371612939435_json',\n",
       " './FO-67381337186147040_json',\n",
       " './FO-134373398370302530_json',\n",
       " './FO-143377828446722768_json',\n",
       " './FO-136713378132396782_json',\n",
       " './FO-313299493256870356_json',\n",
       " './FO-23478819242459063_json',\n",
       " './FO-444942760936986867_json']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(4, 4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1775,  0.6411, -1.0254, -0.3542,  2.2775],\n",
       "         [ 2.1395, -0.0333, -0.7079, -0.0554, -0.1459],\n",
       "         [ 0.2076,  0.1099, -0.0083, -0.3885,  1.0232],\n",
       "         [-0.6194, -0.7919, -1.8684,  0.3775,  1.3191]],\n",
       "\n",
       "        [[ 0.5148,  0.5900, -0.0950,  0.0259,  1.5635],\n",
       "         [-0.3137, -0.6220,  0.0441, -1.6741, -1.5202],\n",
       "         [ 1.1708, -0.0928,  0.6796, -0.8270,  0.5435],\n",
       "         [ 0.4681, -2.1394, -0.2511, -0.5473,  0.9570]],\n",
       "\n",
       "        [[-0.3946, -0.1269,  0.6953,  2.2773, -0.3555],\n",
       "         [ 1.7181, -1.7014, -0.2699,  2.4515, -0.8044],\n",
       "         [ 0.0914, -0.2268,  0.2704, -1.1136, -2.6765],\n",
       "         [-0.1988,  1.7733,  0.1729, -0.5491, -1.5121]],\n",
       "\n",
       "        [[ 0.6188, -0.4705,  0.2686, -0.1335,  0.2923],\n",
       "         [-2.2499,  0.7664, -0.0121,  1.1574, -1.0570],\n",
       "         [-0.3084,  1.6010, -1.1174, -1.4205, -2.2181],\n",
       "         [-0.6761, -0.5214, -0.1795,  0.0117, -1.1420]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.view(4,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1775,  0.6411, -1.0254, -0.3542,  2.2775,  2.1395, -0.0333, -0.7079,\n",
       "         -0.0554, -0.1459,  0.2076,  0.1099, -0.0083, -0.3885,  1.0232, -0.6194,\n",
       "         -0.7919, -1.8684,  0.3775,  1.3191],\n",
       "        [ 0.5148,  0.5900, -0.0950,  0.0259,  1.5635, -0.3137, -0.6220,  0.0441,\n",
       "         -1.6741, -1.5202,  1.1708, -0.0928,  0.6796, -0.8270,  0.5435,  0.4681,\n",
       "         -2.1394, -0.2511, -0.5473,  0.9570],\n",
       "        [-0.3946, -0.1269,  0.6953,  2.2773, -0.3555,  1.7181, -1.7014, -0.2699,\n",
       "          2.4515, -0.8044,  0.0914, -0.2268,  0.2704, -1.1136, -2.6765, -0.1988,\n",
       "          1.7733,  0.1729, -0.5491, -1.5121],\n",
       "        [ 0.6188, -0.4705,  0.2686, -0.1335,  0.2923, -2.2499,  0.7664, -0.0121,\n",
       "          1.1574, -1.0570, -0.3084,  1.6010, -1.1174, -1.4205, -2.2181, -0.6761,\n",
       "         -0.5214, -0.1795,  0.0117, -1.1420]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.2742, -1.5254, -0.4795, -6.7901])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
